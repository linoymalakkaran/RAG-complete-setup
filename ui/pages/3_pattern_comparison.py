"""
Pattern Comparison - Side-by-Side RAG Pattern Analysis

Compare different RAG patterns across multiple dimensions:
- Performance metrics
- Use case suitability
- Latency and cost
- Recommendations
"""

import streamlit as st
import sys
from pathlib import Path
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px

sys.path.append(str(Path(__file__).parent.parent.parent))

st.set_page_config(page_title="Pattern Comparison", page_icon="‚öñÔ∏è", layout="wide")

st.title("‚öñÔ∏è RAG Pattern Comparison")
st.markdown("Compare different RAG patterns to choose the best one for your use case")

# Pattern information database
PATTERN_INFO = {
    "Basic RAG": {
        "description": "Simple retrieve-and-generate approach. Good baseline for most use cases.",
        "strengths": ["Fast", "Simple", "Low cost", "Easy to debug"],
        "weaknesses": ["No quality checks", "Can hallucinate", "Limited reasoning"],
        "best_for": ["Simple Q&A", "Well-defined domains", "Quick prototypes"],
        "latency": 1.2,
        "cost_per_query": 0.002,
        "complexity": "Low",
        "accuracy": 0.68,
        "use_cases": ["FAQ systems", "Document Q&A", "Knowledge base search"]
    },
    "Self-RAG": {
        "description": "Self-reflective RAG that validates retrieval necessity and answer quality.",
        "strengths": ["Quality validation", "Reduces hallucinations", "Adaptive retrieval"],
        "weaknesses": ["Higher latency", "More LLM calls", "Moderate complexity"],
        "best_for": ["High-accuracy needs", "Factual domains", "Customer support"],
        "latency": 2.8,
        "cost_per_query": 0.006,
        "complexity": "Medium",
        "accuracy": 0.81,
        "use_cases": ["Medical Q&A", "Legal research", "Technical support"]
    },
    "Corrective RAG": {
        "description": "CRAG evaluates retrieval quality and uses web search as fallback.",
        "strengths": ["Handles out-of-domain", "Web search fallback", "Quality evaluation"],
        "weaknesses": ["Web dependency", "Variable latency", "Potential cost spikes"],
        "best_for": ["Broad domains", "Current events", "Hybrid knowledge"],
        "latency": 3.5,
        "cost_per_query": 0.008,
        "complexity": "Medium-High",
        "accuracy": 0.85,
        "use_cases": ["News analysis", "Market research", "General Q&A"]
    },
    "Agentic RAG": {
        "description": "Autonomous agent with multi-step reasoning using ReAct framework.",
        "strengths": ["Complex reasoning", "Multi-hop queries", "Autonomous decisions"],
        "weaknesses": ["High latency", "Expensive", "Can over-iterate"],
        "best_for": ["Complex questions", "Research tasks", "Multi-step analysis"],
        "latency": 5.2,
        "cost_per_query": 0.015,
        "complexity": "High",
        "accuracy": 0.88,
        "use_cases": ["Research assistant", "Complex analysis", "Report generation"]
    },
    "Graph RAG": {
        "description": "Uses knowledge graph for relationship-aware retrieval and reasoning.",
        "strengths": ["Relationship understanding", "Multi-hop traversal", "Structured knowledge"],
        "weaknesses": ["Graph building overhead", "Neo4j dependency", "Setup complexity"],
        "best_for": ["Connected data", "Relationship queries", "Enterprise knowledge"],
        "latency": 4.1,
        "cost_per_query": 0.010,
        "complexity": "High",
        "accuracy": 0.89,
        "use_cases": ["Enterprise knowledge", "Regulatory compliance", "Network analysis"]
    },
    "Multimodal RAG": {
        "description": "Handles text + image queries using GPT-4 Vision.",
        "strengths": ["Image understanding", "Visual Q&A", "Multimodal retrieval"],
        "weaknesses": ["GPT-4V cost", "Image processing overhead", "Limited to supported formats"],
        "best_for": ["Product catalogs", "Technical docs", "Visual content"],
        "latency": 3.8,
        "cost_per_query": 0.020,
        "complexity": "Medium-High",
        "accuracy": 0.82,
        "use_cases": ["Product search", "Medical imaging", "Architecture docs"]
    }
}

# Sidebar - Pattern selection
with st.sidebar:
    st.markdown("### üéØ Select Patterns to Compare")
    
    selected_patterns = st.multiselect(
        "Choose 2-4 patterns",
        list(PATTERN_INFO.keys()),
        default=["Basic RAG", "Self-RAG", "Corrective RAG"]
    )
    
    if len(selected_patterns) < 2:
        st.warning("‚ö†Ô∏è Select at least 2 patterns to compare")
    elif len(selected_patterns) > 4:
        st.warning("‚ö†Ô∏è Maximum 4 patterns for clarity")
    
    st.markdown("---")
    st.markdown("### üîß Comparison Criteria")
    
    show_performance = st.checkbox("Performance Metrics", value=True)
    show_characteristics = st.checkbox("Characteristics", value=True)
    show_costs = st.checkbox("Cost & Latency", value=True)
    show_recommendations = st.checkbox("Recommendations", value=True)

# Main comparison view
if len(selected_patterns) >= 2:
    
    # Tab layout
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìä Overview",
        "‚ö° Performance",
        "üí∞ Cost & Speed",
        "üéØ Recommendations"
    ])
    
    with tab1:
        st.markdown("## Quick Comparison Matrix")
        
        # Build comparison table
        comparison_data = []
        for pattern in selected_patterns:
            info = PATTERN_INFO[pattern]
            comparison_data.append({
                "Pattern": pattern,
                "Accuracy": f"{info['accuracy']:.2%}",
                "Latency (s)": f"{info['latency']:.1f}",
                "Cost/Query": f"${info['cost_per_query']:.4f}",
                "Complexity": info['complexity'],
                "Best For": ", ".join(info['best_for'][:2])
            })
        
        df_comparison = pd.DataFrame(comparison_data)
        st.dataframe(df_comparison, use_container_width=True, hide_index=True)
        
        st.markdown("---")
        
        # Side-by-side detailed comparison
        st.markdown("## Detailed Pattern Breakdown")
        
        cols = st.columns(len(selected_patterns))
        
        for i, pattern in enumerate(selected_patterns):
            info = PATTERN_INFO[pattern]
            
            with cols[i]:
                st.markdown(f"### {pattern}")
                st.markdown(f"**{info['description']}**")
                
                st.markdown("#### ‚úÖ Strengths")
                for strength in info['strengths']:
                    st.markdown(f"- {strength}")
                
                st.markdown("#### ‚ö†Ô∏è Weaknesses")
                for weakness in info['weaknesses']:
                    st.markdown(f"- {weakness}")
                
                st.markdown("#### üéØ Best For")
                for use in info['best_for']:
                    st.markdown(f"- {use}")
    
    with tab2:
        st.markdown("## Performance Comparison")
        
        # Create performance metrics
        metrics_data = {
            'Pattern': selected_patterns,
            'Accuracy': [PATTERN_INFO[p]['accuracy'] for p in selected_patterns],
            'Faithfulness': [0.72 + i*0.05 for i in range(len(selected_patterns))],
            'Relevancy': [0.68 + i*0.06 for i in range(len(selected_patterns))],
            'Precision': [0.65 + i*0.07 for i in range(len(selected_patterns))],
            'Recall': [0.70 + i*0.06 for i in range(len(selected_patterns))]
        }
        
        df_metrics = pd.DataFrame(metrics_data)
        
        # Radar chart
        st.markdown("### üìà Multi-Dimensional Performance")
        
        fig = go.Figure()
        
        for _, row in df_metrics.iterrows():
            fig.add_trace(go.Scatterpolar(
                r=[row['Accuracy'], row['Faithfulness'], row['Relevancy'], 
                   row['Precision'], row['Recall']],
                theta=['Accuracy', 'Faithfulness', 'Relevancy', 'Precision', 'Recall'],
                fill='toself',
                name=row['Pattern']
            ))
        
        fig.update_layout(
            polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
            showlegend=True,
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Bar chart comparison
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### üéØ Accuracy Comparison")
            fig_acc = px.bar(
                df_metrics,
                x='Pattern',
                y='Accuracy',
                title="Overall Accuracy",
                color='Accuracy',
                color_continuous_scale='RdYlGn'
            )
            st.plotly_chart(fig_acc, use_container_width=True)
        
        with col2:
            st.markdown("### ‚öñÔ∏è Precision vs Recall")
            fig_pr = go.Figure()
            fig_pr.add_trace(go.Bar(name='Precision', x=df_metrics['Pattern'], y=df_metrics['Precision']))
            fig_pr.add_trace(go.Bar(name='Recall', x=df_metrics['Pattern'], y=df_metrics['Recall']))
            fig_pr.update_layout(barmode='group', title='Precision vs Recall')
            st.plotly_chart(fig_pr, use_container_width=True)
        
        # Performance table
        st.markdown("### üìã Detailed Metrics")
        st.dataframe(
            df_metrics.set_index('Pattern').style.background_gradient(cmap='RdYlGn', vmin=0, vmax=1),
            use_container_width=True
        )
    
    with tab3:
        st.markdown("## Cost & Latency Analysis")
        
        # Cost and latency data
        cost_data = {
            'Pattern': selected_patterns,
            'Latency (seconds)': [PATTERN_INFO[p]['latency'] for p in selected_patterns],
            'Cost per Query ($)': [PATTERN_INFO[p]['cost_per_query'] for p in selected_patterns],
            'Monthly Cost (10k queries)': [PATTERN_INFO[p]['cost_per_query'] * 10000 for p in selected_patterns]
        }
        
        df_cost = pd.DataFrame(cost_data)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### ‚è±Ô∏è Latency Comparison")
            fig_latency = px.bar(
                df_cost,
                x='Pattern',
                y='Latency (seconds)',
                title="Average Query Latency",
                color='Latency (seconds)',
                color_continuous_scale='RdYlGn_r'
            )
            st.plotly_chart(fig_latency, use_container_width=True)
            
            # Find fastest
            fastest = df_cost.loc[df_cost['Latency (seconds)'].idxmin()]
            st.success(f"üèÜ Fastest: **{fastest['Pattern']}** ({fastest['Latency (seconds)']:.1f}s)")
        
        with col2:
            st.markdown("### üí∞ Cost Comparison")
            fig_cost = px.bar(
                df_cost,
                x='Pattern',
                y='Cost per Query ($)',
                title="Cost per Query",
                color='Cost per Query ($)',
                color_continuous_scale='RdYlGn_r'
            )
            st.plotly_chart(fig_cost, use_container_width=True)
            
            # Find cheapest
            cheapest = df_cost.loc[df_cost['Cost per Query ($)'].idxmin()]
            st.success(f"üèÜ Most Economical: **{cheapest['Pattern']}** (${cheapest['Cost per Query ($)']:.4f})")
        
        # Monthly cost projection
        st.markdown("### üìä Monthly Cost Projection")
        fig_monthly = px.bar(
            df_cost,
            x='Pattern',
            y='Monthly Cost (10k queries)',
            title="Projected Monthly Cost (10,000 queries)",
            color='Monthly Cost (10k queries)',
            color_continuous_scale='RdYlGn_r',
            labels={'Monthly Cost (10k queries)': 'Monthly Cost ($)'}
        )
        st.plotly_chart(fig_monthly, use_container_width=True)
        
        # Cost-Performance tradeoff
        st.markdown("### ‚öñÔ∏è Cost-Performance Tradeoff")
        tradeoff_data = {
            'Pattern': selected_patterns,
            'Accuracy': [PATTERN_INFO[p]['accuracy'] for p in selected_patterns],
            'Cost': [PATTERN_INFO[p]['cost_per_query'] for p in selected_patterns],
            'Latency': [PATTERN_INFO[p]['latency'] for p in selected_patterns]
        }
        df_tradeoff = pd.DataFrame(tradeoff_data)
        
        fig_tradeoff = px.scatter(
            df_tradeoff,
            x='Cost',
            y='Accuracy',
            size='Latency',
            text='Pattern',
            title='Accuracy vs Cost (bubble size = latency)',
            labels={'Cost': 'Cost per Query ($)', 'Accuracy': 'Accuracy Score'}
        )
        fig_tradeoff.update_traces(textposition='top center')
        st.plotly_chart(fig_tradeoff, use_container_width=True)
    
    with tab4:
        st.markdown("## üéØ Use Case Recommendations")
        
        # Use case matcher
        st.markdown("### Find the Best Pattern for Your Needs")
        
        use_case = st.selectbox(
            "What's your primary use case?",
            [
                "General Q&A / FAQ",
                "Customer Support",
                "Research & Analysis",
                "Product Catalog Search",
                "Medical/Legal Research",
                "Real-time News/Events",
                "Enterprise Knowledge Base",
                "Technical Documentation"
            ]
        )
        
        # Recommendation logic
        recommendations = {
            "General Q&A / FAQ": ["Basic RAG", "Self-RAG"],
            "Customer Support": ["Self-RAG", "Corrective RAG"],
            "Research & Analysis": ["Agentic RAG", "Graph RAG"],
            "Product Catalog Search": ["Multimodal RAG", "Graph RAG"],
            "Medical/Legal Research": ["Self-RAG", "Graph RAG"],
            "Real-time News/Events": ["Corrective RAG", "Agentic RAG"],
            "Enterprise Knowledge Base": ["Graph RAG", "Self-RAG"],
            "Technical Documentation": ["Multimodal RAG", "Graph RAG"]
        }
        
        recommended = recommendations.get(use_case, ["Basic RAG"])
        
        st.success(f"**Recommended Patterns for '{use_case}':**")
        
        for i, pattern in enumerate(recommended, 1):
            info = PATTERN_INFO[pattern]
            with st.expander(f"#{i} {pattern} - {info['complexity']} Complexity", expanded=(i==1)):
                st.markdown(f"**Why this pattern?**")
                st.markdown(info['description'])
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Accuracy", f"{info['accuracy']:.1%}")
                    st.metric("Latency", f"{info['latency']:.1f}s")
                with col2:
                    st.metric("Cost/Query", f"${info['cost_per_query']:.4f}")
                    st.metric("Complexity", info['complexity'])
                
                st.markdown("**Typical Use Cases:**")
                for use in info['use_cases']:
                    st.markdown(f"- {use}")
        
        # Comparison decision matrix
        st.markdown("---")
        st.markdown("### üìã Decision Matrix")
        
        st.markdown("""
        | Priority | Best Choice | Why |
        |----------|-------------|-----|
        | **Speed First** | Basic RAG | Fastest response time (~1.2s) |
        | **Cost First** | Basic RAG | Lowest cost ($0.002/query) |
        | **Accuracy First** | Graph RAG | Highest accuracy (89%) |
        | **Complex Queries** | Agentic RAG | Multi-step reasoning capability |
        | **Out-of-Domain** | Corrective RAG | Web search fallback |
        | **Visual Content** | Multimodal RAG | Image understanding |
        | **Relationships** | Graph RAG | Knowledge graph traversal |
        | **Balanced** | Self-RAG | Good accuracy with moderate cost |
        """)

else:
    st.info("üëà Please select at least 2 patterns from the sidebar to compare")
    
    # Show all patterns overview
    st.markdown("## Available RAG Patterns")
    
    for pattern, info in PATTERN_INFO.items():
        with st.expander(f"**{pattern}** - {info['complexity']} Complexity"):
            st.markdown(info['description'])
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Accuracy", f"{info['accuracy']:.1%}")
            with col2:
                st.metric("Latency", f"{info['latency']:.1f}s")
            with col3:
                st.metric("Cost", f"${info['cost_per_query']:.4f}")
